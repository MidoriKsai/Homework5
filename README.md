# АНАЛИЗ ДАННЫХ В РАЗРАБОТКЕ ИГР
Отчет по лабораторной работе #5 выполнила:
- Миногина Дарья Алексеевна
- РИ-230932
  
Отметка о выполнении заданий:

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Найдите внутри C# скрипта “коэффициент корреляции” и сделать выводы о том, как он влияет на обучение модели
- Задание 2.
- Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров
- Задание 3.
- Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?
- Выводы.
- ✨Magic✨

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Вводная часть

  В созданный проект был добавлен ML Agent, выбрав Window - Package Manager - Add Package from disk. Последовательно добавлены .json – файлы: ml-agents-release_19 / com,unity.ml-agents / package.json, ml-agents-release_19 / com,unity.ml-agents.extensions / package.json

![Start](https://github.com/MidoriKsai/Homework5/blob/main/start.png)

  Написана серия команд для создания и активации нового ML-агента, а также для скачивания необходимых библиотек

![Сonda](https://github.com/MidoriKsai/Homework5/blob/main/conda.png)

  Созданы на сцене плоскость, куб и сфера, также был подкреплён предоставленный скрипт к сфере и добавлены компоненты Rigidbody, Decision Requester, Behavior Parameters

![Script](https://github.com/MidoriKsai/Homework5/blob/main/script.png)

  Запущено обучение:

![learning](https://github.com/MidoriKsai/Homework5/blob/main/learning.png) 

  Далее было сделано 3, 9, 27 копий модели «Плоскость-Сфера-Куб»

![3platforms](https://github.com/MidoriKsai/Homework5/blob/main/3platforms.png) 

![9](https://github.com/MidoriKsai/Homework5/blob/main/9platforms.png) 

![27platforms](https://github.com/MidoriKsai/Homework5/blob/main/27platforms.png) 

![work](https://github.com/MidoriKsai/Homework5/blob/main/work.png) 

  Результаты:
![results](https://github.com/MidoriKsai/Homework5/blob/main/results.png) 

####Выводы:
На шаге 10,000 Std = 0.493.На шаге 70,000 Std = 0.154.Это свидетельствует о том, что агент демонстрирует всё более стабильные результаты. Средняя награда с шагов 50,000 до 70,000 стабилизируется вблизи 0.976–0.989, что говорит о близости к завершению тренировки или о том, что агент приближается к максимуму, возможному в данной среде.

Далее был скачен и запущен предоставленный проект Unity ML-Agent_EconomicModel

Была остановлена сцена с одним TargetAreaEconomic, запущено обучение для неё

![learn](https://github.com/MidoriKsai/Homework5/blob/main/learn.png)

Для того чтобы ускорить процесс обучения – увеличено количество префабов TargetAreaEconomic, после чего были получены результаты:
![finalResults](https://github.com/MidoriKsai/Homework5/blob/main/finalResults.png) 

Далее были построены графики для оценки результатов обучения с помощью TensorBoard
![TensorBoard](https://github.com/MidoriKsai/Homework5/blob/main/tensorBoard.png) 

## Задание 1
###  Найдите внутри C# скрипта “коэффициент корреляции” и сделать выводы о том, как он влияет на обучение модели

####Выводы о влиянии параметров и логики кода на обучение модели

Параметры, передаваемые в нейронную сеть:

- speedMove: Эта величина определяет скорость передвижения агента. Скорость влияет на время достижения объектов (шахты или деревни), что косвенно влияет на количество выполненных операций за эпизод. Слишком высокая или низкая скорость может привести к неоптимальному использованию времени.
- timeMining: Время добычи золота напрямую связано с задержкой в процессе выполнения задачи. Оно добавляет временные ограничения, влияя на эффективность и стратегию агента.
- amountGold: Количество добываемого золота контролирует доходы агента за цикл. Это ключевой параметр, от которого зависит успешность выполнения задачи.
- pickaxeCost: Стоимость кирки влияет на конечную себестоимость добычи золота, что отражается в вычислениях инфляции. Неоптимальное значение увеличивает расходы, ухудшая результат обучения.
- profitPercentage: Процент прибыли на добычу золота регулирует баланс между доходами и затратами. Он участвует в расчетах экономических показателей, таких как инфляция.

Корреляция между параметрами и инфляцией (tempInf):
Финальной целью агента является поддержание инфляции на уровне ниже 6%. Для этого агент должен находить баланс между значениями pickaxeCost, amountGold и profitPercentage.
Чем точнее агент оптимизирует эти параметры, тем выше вероятность достижения положительного вознаграждения (SetReward(1.0f)).

Логика поведения агента:
Агенты активно перемещаются между деревней и шахтой, выполняя цикл добычи золота и возвращения. Важно, чтобы действия агента, такие как скорость передвижения и время добычи, соответствовали оптимальным условиям для минимизации затрат.
Использование корутин (StartGoldMine, StartMonth) добавляет временные ограничения, которые обеспечивают реалистичность симуляции и необходимость временного планирования у агента.
Цикличность и обучение:

Каждые два месяца агент пересчитывает инфляцию, что становится основным критерием для завершения эпизода. Ошибки в управлении параметрами приведут к отрицательному вознаграждению.
Такая постановка задачи требует от агента не только выполнения базовых действий, но и стратегического управления ресурсами.

Выводы о влиянии на обучение:
Чем точнее агент сможет "научиться" устанавливать параметры speedMove, timeMining, amountGold, pickaxeCost и profitPercentage, тем выше его награда.
Агент проходит процесс обучения, стремясь минимизировать инфляцию и повышать свою прибыль. Это позволяет ему эффективно использовать стратегии экономического управления.
Основным фактором успеха модели является способность агента находить баланс между затратами и прибылью, что моделируется через множество итераций с учетом обратной связи (вознаграждения).
Таким образом, модель на основе взаимодействия с окружением обучается оптимизации экономических и временных параметров. Этот подход позволяет агенту принимать решения, основанные на наблюдениях и прошлых опытах, а также развивает навыки стратегического планирования.

## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров

Проанализируем в формате: текущее значение, изменённое, описание, влияние

batch_size:
Исходное значение: 1024.
Изменим на: 512.
Описание: Этот параметр определяет количество примеров из буфера, используемых для одного шага обновления модели.
Влияние: Меньший batch_size уменьшает требования к памяти, но может привести к менее стабильным обновлениям модели и увеличению времени обучения.

hidden_units:
Исходное значение: 128.
Изменим на: 256.
Описание: Указывает количество нейронов в каждом скрытом слое сети.
Влияние: Увеличение количества нейронов повышает способность модели к обучению более сложным зависимостям, но увеличивает затраты на вычисления.

learning_rate:
Исходное значение: 3.0e-4.
Изменим на: 1.0e-4.
Описание: Скорость обучения определяет, на сколько корректируются веса модели на каждом шаге.
Влияние: Меньшая скорость обучения делает обучение более стабильным, но замедляет процесс достижения хороших результатов. Слишком большая скорость может привести к скачкам и нестабильности.

buffer_size:
Исходное значение: 10240.
Изменим на: 20480.
Описание: Размер буфера, который хранит опыт агента перед обновлением модели.
Влияние: Увеличение буфера позволяет собирать больше данных для обновлений, что улучшает стабильность и качество обучения. Однако это увеличивает требование к памяти и может замедлить частоту обновлений.

gamma:
Исходное значение: 0.99.
Изменим на: 0.95.
Описание: Коэффициент дисконтирования, который определяет, как сильно агент учитывает будущие вознаграждения.
Влияние: Уменьшение gamma делает агента более ориентированным на краткосрочные вознаграждения, что может быть полезно в задачах с быстрым результатом. Увеличение gamma помогает учитывать долгосрочные стратегии.

time_horizon:
Исходное значение: 64.
Изменим на: 128.
Описание: Количество шагов, которые агент использует для вычисления накопленного вознаграждения перед обновлением модели.
Влияние: Увеличение time_horizon позволяет учитывать большее количество шагов, что помогает моделям, где действия агента влияют на долгосрочный результат. Однако это может замедлить обучение из-за увеличения сложности расчётов.

## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?

Оптимизация пути персонажа:
Агент обучается находить оптимальный путь к цели, избегая препятствий или минимизируя время перемещения. Это полезно в динамичных сценариях, где условия могут постоянно меняться.

Адаптивное поведение врагов:
Использование ML-Agent позволяет создавать врагов, которые реагируют на действия игрока, адаптируются к его стратегии, и выбирают оптимальные тактики в реальном времени.

Управление экономикой:
Агент обучается балансировать ресурсы, определять оптимальные затраты и доходы, или управлять торговлей и производством с учетом динамики игрового мира.

Симуляция толпы или NPC:
Машинное обучение может использоваться для создания реалистичного поведения группы персонажей, имитируя решения на основе окружающей среды и взаимодействий с другими агентами.

Ещё возможно автоматизация игрового тестирования:
Агенты могут использоваться для автоматического прохождения игры, тестирования различных сценариев, поиска багов или проверки сложности уровней.

## Выводы
В ходе лабораторной работы изучены основные принципы использования ML-Agent в Unity. Были настроены параметры обучения модели и рассмотрено их влияние на результаты. Проанализированы задачи, где применение ML-Agents упрощает разработку. Изучение параметров yaml-файла позволило понять, как изменения конфигурации влияют на скорость и стабильность обучения. Работа показала эффективность машинного обучения для создания интеллектуальных игровых механик.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
