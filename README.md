# АНАЛИЗ ДАННЫХ В РАЗРАБОТКЕ ИГР
Отчет по лабораторной работе #5 выполнила:
- Миногина Дарья Алексеевна
- РИ-230932
  
Отметка о выполнении заданий:

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Найдите внутри C# скрипта “коэффициент корреляции” и сделать выводы о том, как он влияет на обучение модели
- Задание 2.
- Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров
- Задание 3.
- Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?
- Выводы.
- ✨Magic✨

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Вводная часть

  В созданный проект был добавлен ML Agent, выбрав Window - Package Manager - Add Package from disk. Последовательно добавлены .json – файлы: ml-agents-release_19 / com,unity.ml-agents / package.json, ml-agents-release_19 / com,unity.ml-agents.extensions / package.json

![Start](https://github.com/MidoriKsai/Homework5/blob/main/start.png)

  Написана серия команд для создания и активации нового ML-агента, а также для скачивания необходимых библиотек

![Сonda](https://github.com/MidoriKsai/Homework5/blob/main/conda.png)

  Созданы на сцене плоскость, куб и сфера, также был подкреплён предоставленный скрипт к сфере и добавлены компоненты Rigidbody, Decision Requester, Behavior Parameters

![Script](https://github.com/MidoriKsai/Homework5/blob/main/script.png)

  Запущено обучение:

![learning](https://github.com/MidoriKsai/Homework5/blob/main/learning.png) 

  Далее было сделано 3, 9, 27 копий модели «Плоскость-Сфера-Куб»

![3platforms](https://github.com/MidoriKsai/Homework5/blob/main/3platforms.png) 

![9](https://github.com/MidoriKsai/Homework5/blob/main/9platforms.png) 

![27platforms](https://github.com/MidoriKsai/Homework5/blob/main/27platforms.png) 

![work](https://github.com/MidoriKsai/Homework5/blob/main/work.png) 

  Результаты:
![results](https://github.com/MidoriKsai/Homework5/blob/main/results.png) 

####Выводы:
На шаге 10,000 Std = 0.493.На шаге 70,000 Std = 0.154.Это свидетельствует о том, что агент демонстрирует всё более стабильные результаты. Средняя награда с шагов 50,000 до 70,000 стабилизируется вблизи 0.976–0.989, что говорит о близости к завершению тренировки или о том, что агент приближается к максимуму, возможному в данной среде.

Далее был скачен и запущен предоставленный проект Unity ML-Agent_EconomicModel

Была остановлена сцена с одним TargetAreaEconomic, запущено обучение для неё

![learn](https://github.com/MidoriKsai/Homework5/blob/main/learn.png)

Для того чтобы ускорить процесс обучения – увеличено количество префабов TargetAreaEconomic, после чего были получены результаты:
![finalResults](https://github.com/MidoriKsai/Homework5/blob/main/finalResults.png) 

Далее были построены графики для оценки результатов обучения с помощью TensorBoard
![TensorBoard](https://github.com/MidoriKsai/Homework5/blob/main/tensorBoard.png) 

## Задание 1

  tempInf в этом скрипте — это коэффициент, который измеряет изменения в ценах на золото между двумя месяцами. Агент должен минимизировать этот показатель, чтобы инфляция оставалась ниже 6%, что является его основной целью. Если инфляция (tempInf) слишком высока, агент получает отрицательное вознаграждение, если низкая — положительное. Это побуждает агента оптимизировать такие параметры, как скорость передвижения, время добычи золота, количество добытого золота и стоимость кирки. Агент учится регулировать эти параметры, чтобы снизить инфляцию и увеличить свою прибыль. В процессе обучения агент получает обратную связь, что помогает ему улучшать свою стратегию и принимать более правильные решения. Чем точнее агент настраивает параметры, тем выше его вознаграждение и лучше результаты. В итоге, tempInf помогает агенту понять, как его действия влияют на экономику, и научиться достигать оптимального баланса для минимизации затрат и максимизации прибыли.

## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров

Проанализируем в формате: текущее значение, изменённое, описание, влияние

batch_size:
Исходное значение: 1024.
Изменим на: 512.
Описание: Этот параметр определяет количество примеров из буфера, используемых для одного шага обновления модели.
Влияние: Меньший batch_size уменьшает требования к памяти, но может привести к менее стабильным обновлениям модели и увеличению времени обучения.

hidden_units:
Исходное значение: 128.
Изменим на: 256.
Описание: Указывает количество нейронов в каждом скрытом слое сети.
Влияние: Увеличение количества нейронов повышает способность модели к обучению более сложным зависимостям, но увеличивает затраты на вычисления.

learning_rate:
Исходное значение: 3.0e-4.
Изменим на: 1.0e-4.
Описание: Скорость обучения определяет, на сколько корректируются веса модели на каждом шаге.
Влияние: Меньшая скорость обучения делает обучение более стабильным, но замедляет процесс достижения хороших результатов. Слишком большая скорость может привести к скачкам и нестабильности.

## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?

Оптимизация пути персонажа:
Агент обучается находить оптимальный путь к цели, избегая препятствий или минимизируя время перемещения. Это полезно в динамичных сценариях, где условия могут постоянно меняться.

Адаптивное поведение врагов:
Использование ML-Agent позволяет создавать врагов, которые реагируют на действия игрока, адаптируются к его стратегии, и выбирают оптимальные тактики в реальном времени.

Управление экономикой:
Агент обучается балансировать ресурсы, определять оптимальные затраты и доходы, или управлять торговлей и производством с учетом динамики игрового мира.

Симуляция толпы или NPC:
Машинное обучение может использоваться для создания реалистичного поведения группы персонажей, имитируя решения на основе окружающей среды и взаимодействий с другими агентами.

Ещё возможно автоматизация игрового тестирования:
Агенты могут использоваться для автоматического прохождения игры, тестирования различных сценариев, поиска багов или проверки сложности уровней.

## Выводы
В ходе лабораторной работы изучены основные принципы использования ML-Agent в Unity. Были настроены параметры обучения модели и рассмотрено их влияние на результаты. Проанализированы задачи, где применение ML-Agents упрощает разработку. Изучение параметров yaml-файла позволило понять, как изменения конфигурации влияют на скорость и стабильность обучения. Работа показала эффективность машинного обучения для создания интеллектуальных игровых механик.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
